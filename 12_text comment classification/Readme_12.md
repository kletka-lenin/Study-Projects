# Проект для "Викишоп" с BERT

Интернет-магазин "Викишоп" запускает сервис, предоставляющий пользователям публиковать и дополнять описания товаров, комментировать изменения других пользователей. 

**Цель проекта** - разработать модель для поиска токсичных комментариев, чтобы можно было отправлять их на модерацию.

**Задачи проекта**:
* подготовить данные (было предложено 159 тыс. размеченных комментариев),
* обучить разные модели,
* добиться качества определения токсичных комментариев не менее 0,75 по F1.

Решено было попробовать достичь цели двумя способами: с помощью BERT и с помощью TF-IDF. Использовались следующие библиотеки: Pandas, Numpy, Natural Language Toolkit, Scikit-learn, XGBoost, Pytorch, Transformers.

**Выводы:**

Для решения задачи (выявления токсичных комментариев) применялись два подхода:
* Токенизация текстов комментариев с помощью TF-IDF и затем классификация с помощью линейной регрессии и классификатора XGBoost,
* Токенизация текстов с помощью предобученной модели BERT, создание эмбеддингов и затем классификация с помощью тех же двух моделей.

Для работы по первому сценарию из общего массива данных был выбран сэмпл размером 80000 строк, для второго сценария – 2000 строк.

В данном случае значительной проблемой стала несбалансированность классов: естественно, токсичные комментарии появляются сравнительно редко: в исследуемом случае их было всего около 10% от общего количества.

В рамках первого сценария удалось добиться порогового уровня качества по метрике f1 при использовании логистической регрессии - для этого модели необходимо было указать на наличие дисбаланса классов, а также подобрать оптимальное значение силы регуляризации. После устранения дисбаланса с помощью downsampling качество предсказаний снизилось у обеих моделей. Большое влияние на качество оказывает размер выборки.

В рамках второго сценария ещё одной проблемой стала длина комментариев: выбранная предобученная модель BERT может работать с текстами не длиннее 512 слов, тогда как в данных попадались наблюдения с гораздо большим количеством слов. Решено было ограничить максимальную длину токенов 500. 
По всей видимости, дисбаланс классов сказался на результатах работы моделей и в случае эмбеддингов: ни одной из двух моделей классификации не удалось показать качество выше, чем в случае использования признаков TF-IDF.
